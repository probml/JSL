
import chex

import jax.numpy as jnp
from jax import random, vmap, jit
import haiku as hk

from gym import Env, spaces
from typing import Callable, Optional, Tuple

import base



def make_poly_fit_fn(
    degree: int,
    key: chex.PRNGKey,
    use_bias: bool = True
):
    w_key, b_key = random.split(key)
    ws = random.normal(w_key, (degree,))
    b = random.normal(b_key, (1,)) if use_bias else 0
    degrees = jnp.arange(degree)

    def fit_fn(x: chex.Array):
        def power_fn(w, x, k):
            return w * x**k 
        
        poly = vmap(power_fn, in_axes=(0, None, 0))(ws, x, degrees)
        return jnp.sum(poly, axis=-1) + b
        
    return fit_fn



class ClassificationMLP(Env):
  """Classification-based environment-based inference."""

  def __init__(self,
              fit_fn: Callable,
              x_train_generator: Callable,
              x_test_generator: Callable,
              prior_knowledge: base.PriorKnowledge,
              train_batch_size: int,
              test_batch_size:int,  
              num_steps:  int,
              key: chex.PRNGKey,
              override_train_data: Optional[Tuple[chex.Array, chex.Array]] = None):

    assert prior_knowledge.num_classes > 1

    # Key sequences
    rng = hk.PRNGSequence(key)

    self.fit_fn = fit_fn
    self.tau = prior_knowledge.tau
    self.x_test_generator = x_test_generator
    self.num_steps = num_steps

    input_dim = prior_knowledge.input_dim

    # Optionally override training data where you want to allow for training
    # data that was *not* generated by the x_generator, fit_fn.

    if override_train_data is None:
      nsamples = num_steps * train_batch_size
      (x_train, y_train), _ = base.sample_gaussian_data(
          fit_fn, x_train_generator,
          nsamples, next(rng))
      self.x_train = x_train.reshape((-1, train_batch_size, input_dim))
      self.y_train =  y_train.reshape((-1, train_batch_size, 1))

    else:
      x_train, y_train = override_train_data
      
      x_train = x_train.reshape((-1, train_batch_size, input_dim))
      y_train =  y_train.reshape((-1, train_batch_size, 1))

      assert train_batch_size == x_train.shape[1]
      assert train_batch_size == y_train.shape[1]
      self.train_data = (x_train, y_train)


    (x_test, y_test), _ = base.sample_gaussian_data(
            fit_fn, x_train_generator,
            nsamples, next(rng))
    self.x_test = x_test.reshape((-1, test_batch_size, input_dim))
    self.y_test =  y_test.reshape((-1, test_batch_size, 1))

    
    # Environment OpenAI metadata
    self.reward_range = spaces.Box(low=-jnp.inf, high=jnp.inf, 
                                   shape=(train_batch_size, 1), dtype=jnp.float64)
    self.action_space = spaces.MultiDiscrete([prior_knowledge.num_classes] * train_batch_size)
    self.observation_space = {
                              "X_train":spaces.Box(low=-jnp.inf, high=jnp.inf, 
                                        shape=(train_batch_size, input_dim), dtype=jnp.float64),
                              "Y_train":spaces.Box(low=-jnp.inf, high=jnp.inf, 
                                        shape=(train_batch_size, 1), dtype=jnp.float64),
                              "X_test": spaces.Box(low=-jnp.inf, high=jnp.inf, 
                                        shape=(test_batch_size, input_dim), dtype=jnp.float64),
                              "Y_test": spaces.Box(low=-jnp.inf, high=jnp.inf, 
                                        shape=(test_batch_size, 1), dtype=jnp.float64)
                            }
    self.t = 0
    self.train_batch_size = train_batch_size
    self.test_batch_size = test_batch_size


  @property
  def done(self):
    return  self.t >= self.x_train.shape[0]

  def step(self, action):
    done = self.done
    info = {}

    y = self.y_train[self.t]
    reward = base.cos_sim(action, y)
    self.t += 1
    
    if done:
      observation = {}
    else:
      observation = { "X_train": self.x_train[self.t],
                      "Y_train": self.y_train[self.t],
                      "X_test": self.x_test[self.t],
                      "Y_test": self.y_test[self.t]
                      }
    return observation, reward, done, info


  def test_data(self, key: chex.PRNGKey):
    """Generates test data and evaluates log likelihood w.r.t. environment.
    The test data that is output will be of length tau examples.
    We wanted to "pass" tau here... but ran into jit issues.
    Args:
      key: Random number generator key.
    Returns:
      Tuple of data (with tau examples) and log-likelihood under posterior.
    """
    def sample_test(k: chex.PRNGKey):
      return base.sample_gaussian_data(
          self.fit_fn, self.x_test_generator, self.tau, key=k)
    return jit(sample_test)(key)


  def render(self):
    pass

